{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "905cc01b-7edb-41fc-a71e-8194c8193380",
   "metadata": {},
   "source": [
    "# ChatBot RAG com documentos PDF\n",
    "---\n",
    "Este notebook cria o RAG para o contexto do chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d115cc-453c-4ab3-b75f-22467f097548",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5a9b4c9-45b0-48a8-9b2e-827275d55a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "# Groq\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "## core\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# pypdf e vectordb\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embeddings e textsplitters\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5080a9b-a2ba-467d-abff-7e6ff9404f90",
   "metadata": {},
   "source": [
    "### CONFIGURA√á√ÉO DA API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f66fb9-0360-4919-a6f6-562d7b48963e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key configurada!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "\n",
    "if not GROQ_API_KEY:\n",
    "    raise ValueError(\"GROQ_API_KEY n√£o foi configurada!\")\n",
    "\n",
    "print(\"API Key configurada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe165f46-9132-4d43-b60e-468889f65e44",
   "metadata": {},
   "source": [
    "### CONFIGURA√á√ïES GLOBAIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "07d06873-4c4d-4b2e-b355-a233626209a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CONFIGURA√á√ïES\n",
      "    - Modelo LLM              - openai/gpt-oss-20b\n",
      "    - Temperatura             - 0.1\n",
      "    - Max tokens              - 2048\n",
      "    - Max mensagens hist√≥rico - 20\n",
      "    - Modelo Embeedings:      - sentence-transformers/paraphrase-MiniLM-L6-v2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GROQ_MODELS = {\n",
    "    \"llama-3.3-70b\": \"llama-3.3-70b-versatile\",\n",
    "    \"llama-3.1-8b\": \"llama-3.1-8b-instant\",\n",
    "    \"kimi-k2-instruct\": \"moonshotai/kimi-k2-instruct\",\n",
    "    \"gpt-oss-20b\": \"openai/gpt-oss-20b\",\n",
    "    \"qwen3-32b\": \"qwen/qwen3-32b\"\n",
    "}\n",
    "CONFIGS = {\n",
    "    \"model\":GROQ_MODELS['gpt-oss-20b'],\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 2048,\n",
    "    \"max_messages\":20,\n",
    "    \"chunk_size\":1000,\n",
    "    \"chunk_overlap\":200,\n",
    "    \"embeddings_model\":[\"sentence_transformers/all-MiniLM-L6-v2\", \"sentence-transformers/paraphrase-MiniLM-L6-v2\"],\n",
    "    \"retriever_k\":3,\n",
    "    \"retriever_fetch_k\":6\n",
    "}\n",
    "\n",
    "print(f\"\"\"\n",
    "CONFIGURA√á√ïES\n",
    "    - Modelo LLM              - {CONFIGS['model']}\n",
    "    - Temperatura             - {CONFIGS['temperature']}\n",
    "    - Max tokens              - {CONFIGS['max_tokens']}\n",
    "    - Max mensagens hist√≥rico - {CONFIGS['max_messages']}\n",
    "    - Modelo Embeedings:      - {CONFIGS['embeddings_model'][1]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4314a65f-955a-4449-b1f6-4ed164ed2fe8",
   "metadata": {},
   "source": [
    "### FUN√á√ïES AUXILIARES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b91abfb-2fbe-4e9e-8e7c-ce5ac44179d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check conex√£o\n",
    "def test_groq_connection() -> bool:\n",
    "    \"\"\"Testa a conex√£o com a API Groq.\"\"\"\n",
    "    try:\n",
    "        llm = ChatGroq(\n",
    "            model=CONFIGS['model'],\n",
    "            temperature=0,\n",
    "            api_key=GROQ_API_KEY\n",
    "        )\n",
    "        res = llm.invoke('Responda apenas: OK')\n",
    "        if res.content.strip().upper() == 'OK':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f'Erro: {str(e)}')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d97087a-81c1-4a3a-92e8-1bce85e946d9",
   "metadata": {},
   "source": [
    "### CLASSE PRINCIPAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9acef7df-38fc-4fd9-b000-139e38168d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGChatBot:\n",
    "    \"\"\"\n",
    "    ChatBot para conversar atrav√©s da API Groq\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, configs:Dict[str,Any]=CONFIGS):\n",
    "        \"\"\"Inicializa o chatbot\"\"\"\n",
    "        self.configs = configs\n",
    "        self.llm = None\n",
    "        self.history:List[Any] = []\n",
    "\n",
    "        # RAG\n",
    "        self.embeddings = None\n",
    "        self.vector_store = None\n",
    "        self.retriever = None\n",
    "        self.documents_loaded = False\n",
    "\n",
    "        print('=== INICIALIZANDO CHATBOT ===')\n",
    "        self._initialize_llm()\n",
    "        self._initialize_system_prompt()\n",
    "        self._initialize_embeddings()\n",
    "        print('\\nChatBot pronto!')\n",
    "\n",
    "    def _initialize_llm(self):\n",
    "        \"\"\"Inicializa o modelo de linguagem Groq\"\"\"\n",
    "        self.llm = ChatGroq(\n",
    "            model=self.configs['model'],\n",
    "            temperature=self.configs['temperature'],\n",
    "            max_tokens=self.configs['max_tokens'],\n",
    "            api_key=GROQ_API_KEY\n",
    "        )\n",
    "        print(f'Modelo {self.configs[\"model\"]} carregado!')\n",
    "\n",
    "    def _initialize_system_prompt(self):\n",
    "        \"\"\"Mensagem fixa do sistema\"\"\"\n",
    "        system_message = SystemMessage(\n",
    "            content=\"\"\"\n",
    "            Voc√™ √© um assistente prestativo especializado em responder perguntas com base no contexto fornecido.\n",
    "            Use as informa√ß√µes dos documentos para dar respostas precisas e objetivas. \n",
    "            Se n√£o souber a resposta com base no contexto, diga isso honestamente.\n",
    "            \"\"\"\n",
    "        )\n",
    "        self.history.append(system_message)\n",
    "\n",
    "    def _initialize_embeddings(self):\n",
    "        \"\"\"Inicializa o modelo de embeedings\"\"\"\n",
    "        print(f\"Carregando embeddings: {self.configs['embeddings_model'][1]}...\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=self.configs['embeddings_model'][1],\n",
    "            model_kwargs={'device':'cpu'},\n",
    "            encode_kwargs={'normalize_embeddings':True},\n",
    "        )\n",
    "        print('Embeddings carregados!')\n",
    "    \n",
    "    def load_pdf(self, pdf_path: str, save_index: bool = True, index_path: str = 'faiss_index') -> bool:\n",
    "        \"\"\"\n",
    "        Carrega um PDF e cria o √≠ndice vetorial\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f'Carregando PDF: {pdf_path}')\n",
    "            \n",
    "            if os.path.exists(index_path) and os.path.isdir(index_path): # persistido?\n",
    "                print(f'√çndice encontrado em {index_path}')\n",
    "                self.vector_store = FAISS.load_local(\n",
    "                    index_path,\n",
    "                    self.embeddings,  \n",
    "                    allow_dangerous_deserialization=True                    \n",
    "                )\n",
    "                print('√çndice carregado do disco')\n",
    "            else:\n",
    "                loader = PyMuPDFLoader(pdf_path)\n",
    "                documents = loader.load()\n",
    "                print(f'PDF carregado com {len(documents)} p√°ginas')\n",
    "    \n",
    "                text_splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=self.configs['chunk_size'],\n",
    "                    chunk_overlap=self.configs['chunk_overlap'],\n",
    "                    length_function=len,\n",
    "                    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \";\", \" \", \"\"]\n",
    "                )\n",
    "                chunks = text_splitter.split_documents(documents)\n",
    "                print(f'Texto dividido em {len(chunks)} chunks')\n",
    "    \n",
    "                print('Criando √≠ndice vetorial')\n",
    "                self.vector_store = FAISS.from_documents(chunks, self.embeddings)\n",
    "                print('√çndice vetorial criado!')\n",
    "    \n",
    "                if save_index:\n",
    "                    self.vector_store.save_local(index_path)\n",
    "                    print(f'√çndice salvo em {index_path}')\n",
    "    \n",
    "            # retriever\n",
    "            self.retriever = self.vector_store.as_retriever(\n",
    "                search_type='mmr',\n",
    "                search_kwargs={\n",
    "                    'k': self.configs['retriever_k'],\n",
    "                    'fetch_k': self.configs['retriever_fetch_k']\n",
    "                }\n",
    "            )\n",
    "            self.documents_loaded = True  \n",
    "            print('Sistema RAG pronto para uso!')\n",
    "            return True\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f'Erro ao carregar PDF: {str(e)}')\n",
    "            return False\n",
    "\n",
    "    def retrieve_context(self, query:str) -> str:\n",
    "        \"\"\"\n",
    "        Recupera o contexto relevante dos documentos.\n",
    "\n",
    "        Args:\n",
    "            query: pergunta do usuario\n",
    "        Returns:\n",
    "            string com o contexto concatenado\n",
    "        \"\"\"\n",
    "        if not self.documents_loaded or not self.retriever:\n",
    "            return ''\n",
    "        \n",
    "        docs = self.retriever.invoke(query)\n",
    "        if not docs:\n",
    "            return ''\n",
    "\n",
    "        context_parts=[]\n",
    "        for i, doc in enumerate(docs,1):\n",
    "            source = doc.metadata.get('source', 'Documento')\n",
    "            page = doc.metadata.get('page', 'N/A')\n",
    "            context_parts.append(f'[Documento {i} - P√°gina {page}]:\\n{doc.page_content}')\n",
    "        \n",
    "        return '\\n\\n'.join(context_parts)\n",
    "\n",
    "    def chat(self, user_input: str, use_rag: bool = True, verbose=True):\n",
    "        \"\"\"\n",
    "        Processa um pergunta do usu√°rio e retorna a resposta\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "    \n",
    "        # contexto\n",
    "        if use_rag and self.documents_loaded:\n",
    "            context = self.retrieve_context(user_input)\n",
    "            if context:\n",
    "                enhanced_prompt = f\"\"\"Com base no seguinte contexto, responda √† pergunta do usu√°rio.\n",
    "    Se a resposta n√£o estiver no contexto, diga que n√£o possui essa informa√ß√£o.\n",
    "    \n",
    "    === CONTEXTO ===\n",
    "    {context}\n",
    "    \n",
    "    === PERGUNTA DO USU√ÅRIO ===\n",
    "    {user_input}\n",
    "    \n",
    "    === INSTRU√á√ÉO ===\n",
    "    Responda de forma objetiva usando apenas as informa√ß√µes do contexto acima.\"\"\"\n",
    "                \n",
    "                message = HumanMessage(content=enhanced_prompt)\n",
    "                if verbose:\n",
    "                    print(f'Contexto recuperado: {len(context)} caracteres')\n",
    "            else:\n",
    "                message = HumanMessage(content=user_input)\n",
    "                if verbose:\n",
    "                    print('Sem contexto relevante.')\n",
    "        else:\n",
    "            # sem RAG pergunta direta\n",
    "            message = HumanMessage(content=user_input)\n",
    "    \n",
    "        # apendar hist√≥rico\n",
    "        self.history.append(message)\n",
    "        \n",
    "        # gerar e apendar resposta\n",
    "        res = self.llm.invoke(self.history)\n",
    "        self.history.append(AIMessage(content=res.content))\n",
    "    \n",
    "        # gerenciar hist√≥rico\n",
    "        if len(self.history) > self.configs['max_messages']:\n",
    "            self.history = [self.history[0]] + self.history[-(self.configs['max_messages']-1):]\n",
    "    \n",
    "        elapsed_time = time.time() - start_time\n",
    "    \n",
    "        # output\n",
    "        if verbose:\n",
    "            print(f'\\nUSER: {user_input}')\n",
    "            print(f'\\nBOT: {res.content}')\n",
    "            print(f'\\nTempo: {elapsed_time:.2f}s | Hist√≥rico: {len(self.history)} mensagens')\n",
    "            return None\n",
    "        else: return res.content\n",
    "        #     print(res.content)\n",
    "        \n",
    "        # return res.content\n",
    "    \n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Limpa o hist√≥rico de conversa√ß√£o mantendo o systemp prompt\"\"\"\n",
    "        system_msg = self.history[0] if self.history else None\n",
    "        self.history = [system_msg] if system_msg else []\n",
    "        print('Hist√≥rico limpo')\n",
    "\n",
    "    def clear_documents(self):\n",
    "        \"\"\"Remove todos os documentos carregados\"\"\"\n",
    "        self.vector_store=None\n",
    "        self.retriever=None\n",
    "        self.documents_loaded=False\n",
    "        print('Documentos resolvidos')\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\"Retorna estat√≠sticas do sistema RAG\"\"\"\n",
    "        stats={\n",
    "            'documentos_carregados':self.documents_loaded,\n",
    "            'mensagens_no_historico':len(self.history),\n",
    "            'modelo_llm': self.configs['model'],\n",
    "            'modelo_embeddings': self.configs['embeddings_model'][1]\n",
    "        }\n",
    "        if self.vector_store:\n",
    "            stats['documentos_no_indice']= self.vector_store.index.ntotal\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866f9d4e-ee96-47d8-b37a-453f4785631c",
   "metadata": {},
   "source": [
    "### INICIALIZAR CHATBOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7c0518d7-c63d-44ca-befb-a2ff143cb49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIALIZANDO CHATBOT ===\n",
      "Modelo llama-3.1-8b-instant carregado!\n",
      "Carregando embeddings: sentence-transformers/paraphrase-MiniLM-L6-v2...\n",
      "Embeddings carregados!\n",
      "\n",
      "ChatBot pronto!\n"
     ]
    }
   ],
   "source": [
    "if test_groq_connection():\n",
    "    bot = RAGChatBot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a658f091-effd-4d22-a1ff-128352526446",
   "metadata": {},
   "source": [
    "### FAZER PERGUNTAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bad90c-867e-4e12-af0f-94aa8f49187c",
   "metadata": {},
   "source": [
    "#### CHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cb74eee2-77d4-43ee-8def-6979b99c6a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "USER: Qual a capital da Fran√ßa\n",
      "\n",
      "BOT: A capital da Fran√ßa √© Paris.\n",
      "\n",
      "Tempo: 0.23s | Hist√≥rico: 3 mensagens\n"
     ]
    }
   ],
   "source": [
    "bot.chat('Qual a capital da Fran√ßa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8af87810-b67f-4303-b2af-2025a7c84f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "USER: Resuma econometria em uma frase.\n",
      "\n",
      "BOT: A econometria √© a aplica√ß√£o de m√©todos estat√≠sticos e matem√°ticos para analisar e prever fen√¥menos econ√¥micos, permitindo a tomada de decis√µes informadas em √°reas como pol√≠tica econ√¥mica, gest√£o de empresas e planejamento.\n",
      "\n",
      "Tempo: 1.57s | Hist√≥rico: 5 mensagens\n"
     ]
    }
   ],
   "source": [
    "bot.chat('Resuma econometria em uma frase.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a336456-1db0-413c-9a1c-39af3d7f908a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ü§ñüíªüìäüìàüí°üîç'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.chat('Explique machine learning em uma linha e somente com emojis',verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3aac7114-f46c-4184-b25e-d0a35af2a573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "USER: qual foi minha primeira pergunta?\n",
      "\n",
      "BOT: Sua primeira pergunta foi sobre a capital da Fran√ßa.\n",
      "\n",
      "Tempo: 0.41s | Hist√≥rico: 9 mensagens\n"
     ]
    }
   ],
   "source": [
    "bot.chat('qual foi minha primeira pergunta?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aaf0a3c9-2450-4fc3-a404-000a689e0947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist√≥rico limpo\n"
     ]
    }
   ],
   "source": [
    "bot.clear_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c8e4b24c-53d0-4d7a-8471-5f76741f9884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Essa √© a sua primeira pergunta. Voc√™ perguntou: \"qual foi minha primeira pergunta?\"'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.chat('qual foi minha primeira pergunta?', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dd21783f-a634-4996-9a67-c83aa853ab31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documentos_carregados': False,\n",
       " 'mensagens_no_historico': 3,\n",
       " 'modelo_llm': 'llama-3.1-8b-instant',\n",
       " 'modelo_embeddings': 'sentence-transformers/paraphrase-MiniLM-L6-v2'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.get_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ddb3d1-aab5-4813-af52-51f421f5589d",
   "metadata": {},
   "source": [
    "#### CHAT COM RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "581d48ba-6491-4fcf-bf33-e44250903e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIALIZANDO CHATBOT ===\n",
      "Modelo openai/gpt-oss-20b carregado!\n",
      "Carregando embeddings: sentence-transformers/paraphrase-MiniLM-L6-v2...\n",
      "Embeddings carregados!\n",
      "\n",
      "ChatBot pronto!\n"
     ]
    }
   ],
   "source": [
    "bot_rag = RAGChatBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ac1320e4-500e-4c53-8f4b-ccd7ad9dfe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando PDF: C:\\Users\\wsant\\Downloads\\O'Reilly - PT - SQL Guia PraÃÅtico - Alice Zhao.pdf\n",
      "√çndice encontrado em faiss_index\n",
      "√çndice carregado do disco\n",
      "Sistema RAG pronto para uso!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# carregar pdf\n",
    "bot_rag.load_pdf(\"C:\\\\Users\\\\wsant\\\\Downloads\\\\O'Reilly - PT - SQL Guia PraÃÅtico - Alice Zhao.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6e56363a-c1f2-4411-8e9f-6377c6b71cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexto recuperado: 2241 caracteres\n",
      "\n",
      "USER: Qual o principal tema deste documento?\n",
      "\n",
      "BOT: O principal tema do material apresentado √© **SQL e opera√ß√µes em banco de dados**, abordando consultas, fun√ß√µes de data/hora, manipula√ß√£o de tabelas e conceitos avan√ßados de execu√ß√£o de consultas.\n",
      "\n",
      "Tempo: 0.64s | Hist√≥rico: 3 mensagens\n"
     ]
    }
   ],
   "source": [
    "bot_rag.chat('Qual o principal tema deste documento?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ac3e7914-1731-44f3-ad6b-9512b7cb175d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexto recuperado: 1821 caracteres\n",
      "\n",
      "USER: Quais s√£o as principais conclus√µes ou recomenda√ß√µes?\n",
      "\n",
      "BOT: **Principais conclus√µes / recomenda√ß√µes extra√≠das do contexto**\n",
      "\n",
      "1. **Estrutura e legibilidade do SQL**  \n",
      "   - Use as cl√°usulas padr√£o (SELECT, FROM, WHERE, GROUP‚ÄØBY, HAVING, ORDER‚ÄØBY).  \n",
      "   - A capitaliza√ß√£o e espa√ßos em branco n√£o afetam a execu√ß√£o, mas boas pr√°ticas de formata√ß√£o melhoram a legibilidade.\n",
      "\n",
      "2. **Manuseio de colunas**  \n",
      "   - Evite depender de nomes de colunas fixos em c√≥digo de produ√ß√£o, pois altera√ß√µes na estrutura da tabela podem quebrar o c√≥digo.  \n",
      "   - Prefira selecionar express√µes (ou aliases) que sejam robustas a mudan√ßas de esquema.\n",
      "\n",
      "3. **Agrupamento e agrega√ß√£o**  \n",
      "   - Quando precisar de resultados semelhantes a `GROUP‚ÄØBY`, considere usar fun√ß√µes de janela (`OVER ‚Ä¶ PARTITION BY ‚Ä¶`) com `FIRST_VALUE` como alternativa.\n",
      "\n",
      "4. **Concatena√ß√£o de texto**  \n",
      "   - Para combinar valores de m√∫ltiplas colunas em uma √∫nica coluna, use a fun√ß√£o `CONCAT` ou o operador de concatena√ß√£o `||`.  \n",
      "   - Isso pode ser aplicado tanto a campos de uma mesma linha quanto a campos de v√°rias linhas, conforme o cen√°rio.\n",
      "\n",
      "Tempo: 1.77s | Hist√≥rico: 5 mensagens\n"
     ]
    }
   ],
   "source": [
    "bot_rag.chat(\"Quais s√£o as principais conclus√µes ou recomenda√ß√µes?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "660ef2e8-be0b-4ac7-a477-8e2a0eddf298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexto recuperado: 2175 caracteres\n",
      "\n",
      "USER: Fale sobre o comando SELECT\n",
      "\n",
      "BOT: **Comando SELECT ‚Äì pontos principais extra√≠dos do contexto**\n",
      "\n",
      "| Tema | O que o contexto diz |\n",
      "|------|----------------------|\n",
      "| **Fun√ß√£o b√°sica** | `SELECT` √© usado para listar colunas ou express√µes de uma tabela ou de uma sub‚Äëconsulta. Ex.: `SELECT name, MAX(stop) as num_stops FROM tour GROUP BY name`. |\n",
      "| **Agrupamento** | Pode ser combinado com `GROUP BY` e fun√ß√µes agregadas (`MAX`, `AVG`, etc.). Ex.: `SELECT MAX(num_orders) FROM my_cte`. |\n",
      "| **Sub‚Äëconsultas** | `SELECT` pode ser usado dentro de outra consulta, como em `SELECT AVG(num_stops) FROM (SELECT name, MAX(stop) ‚Ä¶) tour_stops`. |\n",
      "| **Jun√ß√µes** | `SELECT` pode combinar dados de duas ou mais tabelas usando `JOIN`. Ex.: `SELECT * FROM states s INNER JOIN pets p ON s.name = p.name`. |\n",
      "| **Filtros e ordena√ß√£o** | Embora n√£o detalhados explicitamente, o contexto indica que `SELECT` pode ser usado com cl√°usulas `WHERE`, `ORDER BY`, etc., para refinar resultados. |\n",
      "| **Resultado** | O comando retorna linhas que satisfazem a express√£o de sele√ß√£o, podendo incluir agrega√ß√µes, jun√ß√µes ou sub‚Äëconsultas. |\n",
      "\n",
      "Esses pontos resumem como o `SELECT` √© empregado nos exemplos fornecidos.\n",
      "\n",
      "Tempo: 0.89s | Hist√≥rico: 7 mensagens\n"
     ]
    }
   ],
   "source": [
    "bot_rag.chat(\"Fale sobre o comando SELECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5db26123-2857-4cf3-a2d7-cb07920896dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documentos_carregados': True,\n",
       " 'mensagens_no_historico': 7,\n",
       " 'modelo_llm': 'openai/gpt-oss-20b',\n",
       " 'modelo_embeddings': 'sentence-transformers/paraphrase-MiniLM-L6-v2',\n",
       " 'documentos_no_indice': 537}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot_rag.get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9ba0170e-59bc-4207-a894-3652167b3cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos resolvidos\n"
     ]
    }
   ],
   "source": [
    "bot_rag.clear_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "304ac731-aaef-4555-8474-6c4922c2bdb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documentos_carregados': False,\n",
       " 'mensagens_no_historico': 7,\n",
       " 'modelo_llm': 'openai/gpt-oss-20b',\n",
       " 'modelo_embeddings': 'sentence-transformers/paraphrase-MiniLM-L6-v2'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot_rag.get_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
