{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "905cc01b-7edb-41fc-a71e-8194c8193380",
   "metadata": {},
   "source": [
    "# ChatBot RAG com documentos PDF\n",
    "---\n",
    "Este notebook irÃ¡ otimizar a classe para frontend e combina persistencia automatica + contextualizaÃ§Ã£o LCEL com recursos de mÃºltiplos pdfs e merge de indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d115cc-453c-4ab3-b75f-22467f097548",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5a9b4c9-45b0-48a8-9b2e-827275d55a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, Any, List, Optional, Union\n",
    "from datetime import datetime\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "# LLM \n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "## core\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableBranch\n",
    "\n",
    "# pdf loader e vectordb\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embeddings e textsplitters\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5080a9b-a2ba-467d-abff-7e6ff9404f90",
   "metadata": {},
   "source": [
    "### CONFIGURAÃ‡ÃƒO DA API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f66fb9-0360-4919-a6f6-562d7b48963e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key configurada!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "\n",
    "if not GROQ_API_KEY:\n",
    "    raise ValueError(\"GROQ_API_KEY nÃ£o foi configurada!\")\n",
    "\n",
    "print(\"API Key configurada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe165f46-9132-4d43-b60e-468889f65e44",
   "metadata": {},
   "source": [
    "### CONFIGURAÃ‡Ã•ES GLOBAIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "07d06873-4c4d-4b2e-b355-a233626209a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_MODELS = {\n",
    "    \"llama-3.3-70b\": \"llama-3.3-70b-versatile\",\n",
    "    \"llama-3.1-8b\": \"llama-3.1-8b-instant\",\n",
    "    \"kimi-k2\": \"moonshotai/kimi-k2-instruct\",\n",
    "    \"gpt-oss-20b\": \"openai/gpt-oss-20b\",\n",
    "    \"qwen3-32b\": \"qwen/qwen3-32b\"\n",
    "}\n",
    "\n",
    "DEFAULT_CONFIG = {\n",
    "    \"model\": GROQ_MODELS[\"llama-3.1-8b\"],\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 2048,\n",
    "    \"chunk_size\": 1000,\n",
    "    \"chunk_overlap\": 200,\n",
    "    \"embeddings_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"retriever_k\": 4,\n",
    "    \"retriever_fetch_k\": 8,\n",
    "    \"max_history_messages\": 10,\n",
    "    \"index_dir\": \"indices\",\n",
    "}\n",
    "\n",
    "LLM_DEPENDENT = ['model', 'temperature', 'max_tokens']\n",
    "RETRIEVER_DEPENDENT = ['retriever_k', 'retriever_fetch_k']\n",
    "EMBEDDING_DEPENDENT = ['embeddings_model', 'chunk_size', 'chunk_overlap']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d97087a-81c1-4a3a-92e8-1bce85e946d9",
   "metadata": {},
   "source": [
    "### CLASSE PRINCIPAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c77bfd82-3305-4eb9-85b9-d6e9da683487",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGChatBot:\n",
    "    \"\"\"\n",
    "    ChatBot com Lazy Loading e ConfiguraÃ§Ã£o DinÃ¢mica.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Dict[str, Any] = None, api_key: str = None):\n",
    "        self.config = config or DEFAULT_CONFIG.copy()\n",
    "        self.api_key = api_key or os.getenv('GROQ_API_KEY')\n",
    "\n",
    "        if not self.api_key:\n",
    "            raise ValueError('GROQ_API_KEY nÃ£o configurada!')\n",
    "\n",
    "        # componentes\n",
    "        self.llm = None\n",
    "        self.embeddings = None\n",
    "        self.history: List[Any] = []\n",
    "\n",
    "        # indices\n",
    "        self.indices: Dict[str, FAISS] = {}\n",
    "        self.retrievers: Dict[str, Any] = {}\n",
    "\n",
    "        print('\\nInicializando ChatBot...')\n",
    "        self._init_llm()\n",
    "        self._init_embeddings()\n",
    "        self._init_system_prompt()\n",
    "        print('ChatBot pronto!\\n')\n",
    "\n",
    "    def _init_llm(self):\n",
    "        \"\"\"Inicializa LLM.\"\"\"\n",
    "        self.llm = ChatGroq(\n",
    "            model=self.config['model'],\n",
    "            temperature=self.config['temperature'],\n",
    "            max_tokens=self.config['max_tokens'],\n",
    "            api_key=self.api_key\n",
    "        )\n",
    "        print(f'LLM: {self.config[\"model\"]} (temp={self.config[\"temperature\"]}, tokens={self.config[\"max_tokens\"]})')\n",
    "\n",
    "    def _init_embeddings(self):\n",
    "        \"\"\"Inicializa embeddings.\"\"\"\n",
    "        print(f\"Embeddings: {self.config['embeddings_model']}\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=self.config['embeddings_model'],\n",
    "            model_kwargs={'device': 'cpu'},\n",
    "            encode_kwargs={'normalize_embeddings': True},\n",
    "        )\n",
    "\n",
    "    def _init_system_prompt(self):\n",
    "        \"\"\"System prompt.\"\"\"\n",
    "        system_msg = SystemMessage(\n",
    "            content=\"\"\"VocÃª Ã© um assistente especializado em responder com base em documentos fornecidos.\"\"\")\n",
    "        self.history.append(system_msg)\n",
    "\n",
    "    # ========================================================================\n",
    "    # UPDATE CONFIG -> PARA FRONTEND\n",
    "    # ========================================================================\n",
    "\n",
    "    def update_config(self, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Atualiza configuraÃ§Ãµes dinamicamente. Recria componentes afetados.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: Configs a atualizar (model, temperature, max_tokens, \n",
    "                     retriever_k, retriever_fetch_k, etc.)\n",
    "\n",
    "        Returns:\n",
    "            Dict com success, message, changes, recreated, current_config\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            'success': False,\n",
    "            'message': '',\n",
    "            'changes': [],\n",
    "            'recreated': [],\n",
    "            'current_config': {},\n",
    "            'warning': None\n",
    "        }\n",
    "\n",
    "        # validar chaves\n",
    "        valid_keys = set(DEFAULT_CONFIG.keys())\n",
    "        invalid = [k for k in kwargs if k not in valid_keys]\n",
    "        if invalid:\n",
    "            result['message'] = f\"InvÃ¡lidas: {', '.join(invalid)}\"\n",
    "            result['current_config'] = self.get_config()\n",
    "            return result\n",
    "\n",
    "        # validar valores\n",
    "        errors = []\n",
    "        if 'temperature' in kwargs:\n",
    "            t = kwargs['temperature']\n",
    "            if not isinstance(t, (int, float)) or not (0.0 <= t <= 2.0):\n",
    "                errors.append(\"temperature: 0.0-2.0\")\n",
    "\n",
    "        if 'max_tokens' in kwargs:\n",
    "            if not isinstance(kwargs['max_tokens'], int) or kwargs['max_tokens'] < 1:\n",
    "                errors.append(\"max_tokens: inteiro > 0\")\n",
    "\n",
    "        if 'retriever_k' in kwargs:\n",
    "            if not isinstance(kwargs['retriever_k'], int) or kwargs['retriever_k'] < 1:\n",
    "                errors.append(\"retriever_k: inteiro > 0\")\n",
    "\n",
    "        if errors:\n",
    "            result['message'] = \"Erros: \" + \"; \".join(errors)\n",
    "            result['current_config'] = self.get_config()\n",
    "            return result\n",
    "\n",
    "        # detectar mudanÃ§as\n",
    "        changes = []\n",
    "        needs_llm = False\n",
    "        needs_retriever = False\n",
    "        needs_embedding = False\n",
    "\n",
    "        for key, new_val in kwargs.items():\n",
    "            old_val = self.config.get(key)\n",
    "            if old_val != new_val:\n",
    "                self.config[key] = new_val\n",
    "                changes.append(f\"{key}: {old_val} â†’ {new_val}\")\n",
    "\n",
    "                if key in LLM_DEPENDENT:\n",
    "                    needs_llm = True\n",
    "                if key in RETRIEVER_DEPENDENT:\n",
    "                    needs_retriever = True\n",
    "                if key in EMBEDDING_DEPENDENT:\n",
    "                    needs_embedding = True\n",
    "\n",
    "        if not changes:\n",
    "            result['success'] = True\n",
    "            result['message'] = \"Nenhuma alteraÃ§Ã£o\"\n",
    "            result['current_config'] = self.get_config()\n",
    "            return result\n",
    "\n",
    "        # aplicar mudanÃ§as\n",
    "        try:\n",
    "            if needs_embedding:\n",
    "                print(\"\\nRecriando embeddings...\")\n",
    "                self._init_embeddings()\n",
    "                result['recreated'].append('embeddings')\n",
    "                # embeddings mudou â†’ Ã­ndices recriados\n",
    "                if self.indices:\n",
    "                    print(\"Modelo de embeddings mudou! Ãndices precisam ser recarregados.\")\n",
    "                    self.indices = {}\n",
    "                    self.retrievers = {}\n",
    "                    result['warning'] = \"Recarregue os documentos (embeddings alterados)\"\n",
    "\n",
    "            if needs_llm:\n",
    "                print(\"Recriando LLM...\")\n",
    "                self._init_llm()\n",
    "                result['recreated'].append('llm')\n",
    "\n",
    "            if needs_retriever and self.indices:\n",
    "                print(\"Recriando retrievers...\")\n",
    "                self._recreate_retrievers()\n",
    "                result['recreated'].append('retrievers')\n",
    "\n",
    "            # recriar chain se LLM ou retriever mudou\n",
    "            if (needs_llm or needs_retriever) and self.indices:\n",
    "                print(\"Recriando RAG chain...\")\n",
    "                self._create_unified_rag_chain()\n",
    "                result['recreated'].append('rag_chain')\n",
    "\n",
    "            result['success'] = True\n",
    "            result['message'] = f\"Atualizado ({len(changes)} mudanÃ§as)\"\n",
    "            result['changes'] = changes\n",
    "\n",
    "        except Exception as e:\n",
    "            result['message'] = f\"Erro: {str(e)}\"\n",
    "\n",
    "        result['current_config'] = self.get_config()\n",
    "        return result\n",
    "\n",
    "    def _recreate_retrievers(self):\n",
    "        \"\"\"Recria todos os retrievers com novos parÃ¢metros.\"\"\"\n",
    "        self.retrievers = {}\n",
    "        for name, vectorstore in self.indices.items():\n",
    "            self.retrievers[name] = vectorstore.as_retriever(\n",
    "                search_type=\"mmr\",\n",
    "                search_kwargs={\n",
    "                    \"k\": self.config[\"retriever_k\"],\n",
    "                    \"fetch_k\": self.config[\"retriever_fetch_k\"]\n",
    "                }\n",
    "            )\n",
    "\n",
    "    def get_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"Retorna config atual.\"\"\"\n",
    "        return self.config.copy()\n",
    "\n",
    "    def reset_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"Reseta para padrÃ£o.\"\"\"\n",
    "        # preservar index_dir e outras configs nÃ£o-LLM\n",
    "        preserved = {k: v for k, v in self.config.items() \n",
    "                    if k not in DEFAULT_CONFIG or k in ['index_dir']}\n",
    "        default = DEFAULT_CONFIG.copy()\n",
    "        default.update(preserved)\n",
    "        return self.update_config(**default)\n",
    "\n",
    "    # ========================================================================\n",
    "    # CARGA PDFs\n",
    "    # ========================================================================\n",
    "\n",
    "    def load_documents(self, pdf_paths: Union[str, List[str]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Carrega lista de PDFs com lazy loading.\n",
    "        Para cada PDF: se Ã­ndice existe carrega, senÃ£o cria.\n",
    "        \"\"\"\n",
    "        if isinstance(pdf_paths, str):\n",
    "            pdf_paths = [pdf_paths]\n",
    "\n",
    "        results = {'loaded': [], 'created': [], 'failed': [], 'total': 0}\n",
    "\n",
    "        print(f\"\\nProcessando {len(pdf_paths)} documento(s)...\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        for pdf_path in pdf_paths:\n",
    "            pdf_path = os.path.abspath(pdf_path)\n",
    "\n",
    "            if not os.path.exists(pdf_path):\n",
    "                print(f\"NÃ£o encontrado: {pdf_path}\")\n",
    "                results['failed'].append({'file': pdf_path, 'error': 'Not found'})\n",
    "                continue\n",
    "\n",
    "            pdf_name = Path(pdf_path).stem\n",
    "            safe_name = self._sanitize_name(pdf_name)\n",
    "            index_path = os.path.join(self.config['index_dir'], safe_name)\n",
    "\n",
    "            print(f\"\\n{pdf_name}\")\n",
    "\n",
    "            try:\n",
    "                if os.path.exists(index_path) and os.path.isdir(index_path):\n",
    "                    # CARREGAR DO DISCO\n",
    "                    print(f\"Carregando Ã­ndice existente...\")\n",
    "                    self._load_single_index(safe_name, index_path)\n",
    "                    results['loaded'].append(pdf_name)\n",
    "                    print(f\"Carregado\")\n",
    "                else:\n",
    "                    # CRIAR NOVO\n",
    "                    print(f\"   ğŸ”§ Criando novo Ã­ndice...\")\n",
    "                    success = self._create_single_index(pdf_path, safe_name, index_path)\n",
    "                    if success:\n",
    "                        results['created'].append(pdf_name)\n",
    "                        print(f\"Criado\")\n",
    "                    else:\n",
    "                        results['failed'].append({'file': pdf_name, 'error': 'Process failed'})\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erro: {str(e)}\")\n",
    "                results['failed'].append({'file': pdf_name, 'error': str(e)})\n",
    "\n",
    "        results['total'] = len(self.indices)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"Resumo: {len(results['loaded'])} carregados, {len(results['created'])} criados, {len(results['failed'])} falhas\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        if self.indices:\n",
    "            self._create_unified_rag_chain()\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _sanitize_name(self, name: str) -> str:\n",
    "        \"\"\"Nome seguro para pasta.\"\"\"\n",
    "        safe = \"\".join(c if c.isalnum() or c in ('-', '_') else '_' for c in name)\n",
    "        return safe[:50]\n",
    "\n",
    "    def _create_single_index(self, pdf_path: str, name: str, index_path: str) -> bool:\n",
    "        \"\"\"Cria Ã­ndice para um PDF.\"\"\"\n",
    "        os.makedirs(self.config['index_dir'], exist_ok=True)\n",
    "\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "\n",
    "        if not docs:\n",
    "            return False\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.config[\"chunk_size\"],\n",
    "            chunk_overlap=self.config[\"chunk_overlap\"],\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \";\", \" \", \"\"]\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "        if not chunks:\n",
    "            return False\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunk.metadata['source_file'] = Path(pdf_path).name\n",
    "            chunk.metadata['source_name'] = name\n",
    "\n",
    "        vectorstore = FAISS.from_documents(chunks, self.embeddings)\n",
    "        vectorstore.save_local(index_path)\n",
    "\n",
    "        # metadados\n",
    "        metadata = {\n",
    "            'original_file': pdf_path,\n",
    "            'name': name,\n",
    "            'chunks': len(chunks),\n",
    "            'pages': len(docs),\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'config': {\n",
    "                'embeddings_model': self.config['embeddings_model'],\n",
    "                'chunk_size': self.config['chunk_size']\n",
    "            }\n",
    "        }\n",
    "        with open(os.path.join(index_path, 'info.json'), 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "\n",
    "        self.indices[name] = vectorstore\n",
    "        self.retrievers[name] = vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\"k\": self.config[\"retriever_k\"], \"fetch_k\": self.config[\"retriever_fetch_k\"]}\n",
    "        )\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _load_single_index(self, name: str, index_path: str):\n",
    "        \"\"\"Carrega Ã­ndice existente.\"\"\"\n",
    "        vectorstore = FAISS.load_local(\n",
    "            index_path,\n",
    "            self.embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "\n",
    "        self.indices[name] = vectorstore\n",
    "        self.retrievers[name] = vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\"k\": self.config[\"retriever_k\"], \"fetch_k\": self.config[\"retriever_fetch_k\"]}\n",
    "        )\n",
    "\n",
    "    # ========================================================================\n",
    "    # RAG CHAIN\n",
    "    # ========================================================================\n",
    "\n",
    "    def _search_all_indices(self, query: str) -> List[Document]:\n",
    "        \"\"\"Busca em TODOS os Ã­ndices.\"\"\"\n",
    "        all_docs = []\n",
    "        for name, retriever in self.retrievers.items():\n",
    "            docs = retriever.invoke(query)\n",
    "            for doc in docs:\n",
    "                doc.metadata['index_source'] = name\n",
    "            all_docs.extend(docs)\n",
    "        return all_docs\n",
    "\n",
    "    def _create_unified_rag_chain(self):\n",
    "        \"\"\"Cria chain que busca em todos os Ã­ndices.\"\"\"\n",
    "\n",
    "        contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"Reformule considerando histÃ³rico. Mantenha portuguÃªs.\"),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ])\n",
    "\n",
    "        contextualize_chain = contextualize_q_prompt | self.llm | StrOutputParser()\n",
    "\n",
    "        def get_context(x: dict) -> str:\n",
    "            if x.get(\"chat_history\") and len(x[\"chat_history\"]) > 1:\n",
    "                query = contextualize_chain.invoke(x)\n",
    "            else:\n",
    "                query = x[\"input\"]\n",
    "\n",
    "            docs = self._search_all_indices(query)\n",
    "\n",
    "            formatted = []\n",
    "            for i, doc in enumerate(docs, 1):\n",
    "                page = doc.metadata.get(\"page\", \"?\")\n",
    "                source = doc.metadata.get(\"source_file\", \"?\")\n",
    "                formatted.append(f\"[Doc {i} | {source} | PÃ¡g. {page}]\\n{doc.page_content}\")\n",
    "\n",
    "            return \"\\n\\n---\\n\\n\".join(formatted)\n",
    "\n",
    "        qa_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Responda baseado no contexto (mÃºltiplos documentos).\n",
    "Se nÃ£o souber, diga \"NÃ£o encontrei\".\n",
    "Cite a fonte.\n",
    "\n",
    "Contexto:\n",
    "{context}\"\"\"),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ])\n",
    "\n",
    "        self.rag_chain = (\n",
    "            RunnablePassthrough.assign(context=lambda x: get_context(x))\n",
    "            | qa_prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    # ========================================================================\n",
    "    # CHAT\n",
    "    # ========================================================================\n",
    "\n",
    "    def chat(self, user_input: str, verbose: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Processa pergunta.\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nğŸ‘¤ {user_input}\")\n",
    "\n",
    "        if not self.indices:\n",
    "            # Modo chat puro\n",
    "            self.history.append(HumanMessage(content=user_input))\n",
    "            response = self.llm.invoke(self.history)\n",
    "            self.history.append(AIMessage(content=response.content))\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\nğŸ¤– {response.content}\")\n",
    "\n",
    "            return {\"answer\": response.content, \"mode\": \"CHAT\", \"sources\": []}\n",
    "\n",
    "        # Modo RAG\n",
    "        if verbose:\n",
    "            print(f\"ğŸ” Buscando em {len(self.indices)} documento(s)...\")\n",
    "\n",
    "        response = self.rag_chain.invoke({\n",
    "            \"input\": user_input,\n",
    "            \"chat_history\": self.history\n",
    "        })\n",
    "\n",
    "        sources = self._search_all_indices(user_input)\n",
    "\n",
    "        self.history.append(HumanMessage(content=user_input))\n",
    "        self.history.append(AIMessage(content=response))\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nğŸ¤– {response}\")\n",
    "            print(f\"\\nğŸ“š Fontes: {len(sources)}\")\n",
    "            by_source = {}\n",
    "            for doc in sources[:6]:\n",
    "                src = doc.metadata.get('source_file', '?')\n",
    "                by_source.setdefault(src, []).append(doc.metadata.get('page', '?'))\n",
    "            for src, pages in by_source.items():\n",
    "                print(f\"   ğŸ“„ {src} (pÃ¡gs: {', '.join(map(str, pages[:3]))})\")\n",
    "            print(f\"\\nâ±ï¸  {elapsed:.2f}s\")\n",
    "\n",
    "        self._trim_history()\n",
    "\n",
    "        return {\"answer\": response, \"mode\": \"RAG\", \"sources\": sources, \"time\": elapsed}\n",
    "\n",
    "    def _trim_history(self):\n",
    "        \"\"\"Limita histÃ³rico.\"\"\"\n",
    "        max_msgs = self.config[\"max_history_messages\"] * 2\n",
    "        if len(self.history) > max_msgs + 1:\n",
    "            self.history = [self.history[0]] + self.history[-max_msgs:]\n",
    "\n",
    "    # ========================================================================\n",
    "    # UTILITÃRIOS\n",
    "    # ========================================================================\n",
    "\n",
    "    def list_loaded_documents(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Lista documentos carregados.\"\"\"\n",
    "        docs = []\n",
    "        for name in self.indices.keys():\n",
    "            index_path = os.path.join(self.config['index_dir'], name)\n",
    "            info_path = os.path.join(index_path, 'info.json')\n",
    "\n",
    "            info = {}\n",
    "            if os.path.exists(info_path):\n",
    "                with open(info_path, 'r') as f:\n",
    "                    info = json.load(f)\n",
    "\n",
    "            docs.append({\n",
    "                'name': name,\n",
    "                'original_file': info.get('original_file', '?'),\n",
    "                'chunks': info.get('chunks', 0),\n",
    "                'pages': info.get('pages', 0),\n",
    "                'created': info.get('created_at', '?')\n",
    "            })\n",
    "        return docs\n",
    "\n",
    "    def remove_document(self, name: str) -> bool:\n",
    "        \"\"\"Remove documento da memÃ³ria.\"\"\"\n",
    "        if name not in self.indices:\n",
    "            print(f\"'{name}' nÃ£o carregado\")\n",
    "            return False\n",
    "\n",
    "        del self.indices[name]\n",
    "        del self.retrievers[name]\n",
    "\n",
    "        if self.indices:\n",
    "            self._create_unified_rag_chain()\n",
    "        else:\n",
    "            self.rag_chain = None\n",
    "\n",
    "        print(f\"'{name}' removido da memÃ³ria\")\n",
    "        return True\n",
    "\n",
    "    def clear_all(self):\n",
    "        \"\"\"Remove todos da memÃ³ria.\"\"\"\n",
    "        self.indices = {}\n",
    "        self.retrievers = {}\n",
    "        self.rag_chain = None\n",
    "        print(\"MemÃ³ria limpa\")\n",
    "\n",
    "    def delete_index(self, name: str) -> bool:\n",
    "        \"\"\"Deleta Ã­ndice do disco permanentemente.\"\"\"\n",
    "        index_path = os.path.join(self.config['index_dir'], name)\n",
    "\n",
    "        if not os.path.exists(index_path):\n",
    "            print(f\"Ãndice '{name}' nÃ£o existe\")\n",
    "            return False\n",
    "\n",
    "        # remover da memÃ³ria se estiver carregado\n",
    "        if name in self.indices:\n",
    "            self.remove_document(name)\n",
    "\n",
    "        # deletar do disco\n",
    "        shutil.rmtree(index_path)\n",
    "        print(f\"Ãndice '{name}' deletado do disco\")\n",
    "        return True\n",
    "\n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"EstatÃ­sticas.\"\"\"\n",
    "        return {\n",
    "            'documents_in_memory': len(self.indices),\n",
    "            'document_names': list(self.indices.keys()),\n",
    "            'history_messages': len(self.history),\n",
    "            'model': self.config['model'],\n",
    "            'temperature': self.config['temperature'],\n",
    "            'embeddings': self.config['embeddings_model']\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866f9d4e-ee96-47d8-b37a-453f4785631c",
   "metadata": {},
   "source": [
    "### INICIALIZAR CHATBOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7c0518d7-c63d-44ca-befb-a2ff143cb49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Inicializando RAGChatBot Pro...\n",
      "ğŸ¤– LLM: llama-3.1-8b-instant (temp=0.1, tokens=2048)\n",
      "ğŸ“Š Embeddings: sentence-transformers/all-MiniLM-L6-v2\n",
      "âœ… Bot pronto!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bot = RAGChatBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a1801ff-4347-4df0-aa02-e77a2066a0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents_in_memory': 0,\n",
       " 'document_names': [],\n",
       " 'history_messages': 1,\n",
       " 'model': 'llama-3.1-8b-instant',\n",
       " 'temperature': 0.1,\n",
       " 'embeddings': 'sentence-transformers/all-MiniLM-L6-v2'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.get_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a658f091-effd-4d22-a1ff-128352526446",
   "metadata": {},
   "source": [
    "### FAZER PERGUNTAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bad90c-867e-4e12-af0f-94aa8f49187c",
   "metadata": {},
   "source": [
    "#### CHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cb74eee2-77d4-43ee-8def-6979b99c6a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘¤ Qual a capital da FranÃ§a\n",
      "\n",
      "ğŸ¤– A capital da FranÃ§a Ã© Paris.\n"
     ]
    }
   ],
   "source": [
    "bot.chat('Qual a capital da FranÃ§a');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8af87810-b67f-4303-b2af-2025a7c84f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘¤ Resuma econometria em uma frase curta e provocante.\n",
      "\n",
      "ğŸ¤– \"A econometria Ã© a arte de encontrar padrÃµes ocultos em nÃºmeros, mas nem sempre Ã© fÃ¡cil descobrir o que eles estÃ£o tentando nos dizer.\"\n"
     ]
    }
   ],
   "source": [
    "bot.chat('Resuma econometria em uma frase curta e provocante.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8a336456-1db0-413c-9a1c-39af3d7f908a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'ğŸ¤–ğŸ’»ğŸ“ŠğŸ“ˆğŸ’¡ğŸ”', 'mode': 'CHAT', 'sources': []}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.chat('Explique machine learning em uma linha e somente com emojis',verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3aac7114-f46c-4184-b25e-d0a35af2a573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sua primeira pergunta foi sobre a capital da FranÃ§a.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = bot.chat('qual foi minha primeira pergunta?', verbose=False)\n",
    "res['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ddb3d1-aab5-4813-af52-51f421f5589d",
   "metadata": {},
   "source": [
    "#### CHAT COM RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d345d8e5-97b8-461a-a648-59632d8ba235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents_in_memory': 0,\n",
       " 'document_names': [],\n",
       " 'history_messages': 9,\n",
       " 'model': 'llama-3.1-8b-instant',\n",
       " 'temperature': 0.1,\n",
       " 'embeddings': 'sentence-transformers/all-MiniLM-L6-v2'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.get_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4588d7-0b0e-4237-a32a-9e40828f62d0",
   "metadata": {},
   "source": [
    "##### CARREGANDO UM PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ac1320e4-500e-4c53-8f4b-ccd7ad9dfe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“š Processando 1 documento(s)...\n",
      "============================================================\n",
      "\n",
      "ğŸ“„ O'Reilly - PT - SQL Guia PraÌtico - Alice Zhao\n",
      "   ğŸ”§ Criando novo Ã­ndice...\n",
      "   âœ… Criado\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š Resumo: 0 carregados, 1 criados, 0 falhas\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loaded': [],\n",
       " 'created': [\"O'Reilly - PT - SQL Guia PraÌtico - Alice Zhao\"],\n",
       " 'failed': [],\n",
       " 'total': 1}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.load_documents(\"C:\\\\Users\\\\wsant\\\\Downloads\\\\O'Reilly - PT - SQL Guia PraÌtico - Alice Zhao.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6e56363a-c1f2-4411-8e9f-6377c6b71cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘¤ Qual o principal tema deste documento?\n",
      "ğŸ” Buscando em 1 documento(s)...\n",
      "\n",
      "ğŸ¤– O principal tema deste documento Ã© SQL (Structured Query Language), especificamente um guia prÃ¡tico para aprender e trabalhar com SQL.\n",
      "\n",
      "ğŸ“š Fontes: 4\n",
      "   ğŸ“„ O'Reilly - PT - SQL Guia PraÌtico - Alice Zhao.pdf (pÃ¡gs: 25, 2, 13)\n",
      "\n",
      "â±ï¸  0.54s\n"
     ]
    }
   ],
   "source": [
    "bot.chat('Qual o principal tema deste documento?');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "660ef2e8-be0b-4ac7-a477-8e2a0eddf298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘¤ Explique para um leigo em um parÃ¡grafo sobre o comando SELECT\n",
      "ğŸ” Buscando em 1 documento(s)...\n",
      "\n",
      "ğŸ¤– O comando SELECT Ã© um dos principais comandos do SQL e serve para selecionar os dados que vocÃª deseja ver de uma tabela de banco de dados. Imagine que vocÃª estÃ¡ procurando por uma informaÃ§Ã£o especÃ­fica em um grande livro de notas. O comando SELECT Ã© como uma lupa que permite vocÃª escolher apenas as informaÃ§Ãµes que vocÃª precisa, como o nome, a idade ou a nota de um aluno, por exemplo. Com o SELECT, vocÃª pode especificar quais colunas (campos) da tabela vocÃª deseja ver e, em seguida, o SQL mostrarÃ¡ apenas essas informaÃ§Ãµes, tornando mais fÃ¡cil encontrar o que vocÃª estÃ¡ procurando.\n",
      "\n",
      "ğŸ“š Fontes: 4\n",
      "   ğŸ“„ O'Reilly - PT - SQL Guia PraÌtico - Alice Zhao.pdf (pÃ¡gs: 59, 78, 83)\n",
      "\n",
      "â±ï¸  1.23s\n"
     ]
    }
   ],
   "source": [
    "bot.chat(\"Explique para um leigo em um parÃ¡grafo sobre o comando SELECT\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5db26123-2857-4cf3-a2d7-cb07920896dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents_in_memory': 1,\n",
       " 'document_names': ['O_Reilly_-_PT_-_SQL_Guia_Pra_tico_-_Alice_Zhao'],\n",
       " 'history_messages': 5,\n",
       " 'model': 'llama-3.1-8b-instant',\n",
       " 'temperature': 0.1,\n",
       " 'embeddings': 'sentence-transformers/all-MiniLM-L6-v2'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9ba0170e-59bc-4207-a894-3652167b3cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos removidos!\n"
     ]
    }
   ],
   "source": [
    "bot.clear_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d43e07-804f-45af-b07c-e2136e43d9a7",
   "metadata": {},
   "source": [
    "##### CARREGANDO MULTIPLOS PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "035988f0-6fb1-484f-a1bf-174b1e75535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs = [\n",
    "    \"C:\\\\Users\\\\wsant\\\\Downloads\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf\",\n",
    "    \"C:\\\\Users\\\\wsant\\\\Downloads\\\\000906000101011.pdf\",\n",
    "    \"C:\\\\Users\\\\wsant\\\\Downloads\\\\O'Reilly - PT - SQL Guia PraÌtico - Alice Zhao.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e18b6987-07ad-4366-820c-f80c11fe0b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“š Processando 3 documento(s)...\n",
      "============================================================\n",
      "\n",
      "ğŸ“„ NIPS-2017-attention-is-all-you-need-Paper\n",
      "   ğŸ”§ Criando novo Ã­ndice...\n",
      "   âœ… Criado\n",
      "\n",
      "ğŸ“„ 000906000101011\n",
      "   ğŸ”§ Criando novo Ã­ndice...\n",
      "   âœ… Criado\n",
      "\n",
      "ğŸ“„ O'Reilly - PT - SQL Guia PraÌtico - Alice Zhao\n",
      "   ğŸ’¾ Carregando Ã­ndice existente...\n",
      "   âœ… Carregado\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š Resumo: 1 carregados, 2 criados, 0 falhas\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loaded': [\"O'Reilly - PT - SQL Guia PraÌtico - Alice Zhao\"],\n",
       " 'created': ['NIPS-2017-attention-is-all-you-need-Paper', '000906000101011'],\n",
       " 'failed': [],\n",
       " 'total': 3}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.load_documents(pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "751d98f9-1099-4a31-b564-b0ff02770293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents_in_memory': 3,\n",
       " 'document_names': ['O_Reilly_-_PT_-_SQL_Guia_Pra_tico_-_Alice_Zhao',\n",
       "  'NIPS-2017-attention-is-all-you-need-Paper',\n",
       "  '000906000101011'],\n",
       " 'history_messages': 5,\n",
       " 'model': 'llama-3.1-8b-instant',\n",
       " 'temperature': 0.1,\n",
       " 'embeddings': 'sentence-transformers/all-MiniLM-L6-v2'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "75f64ade-102c-4ed8-9870-543474dccdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘¤ Sobre o que sÃ£o estes documentos?\n",
      "ğŸ” Buscando em 3 documento(s)...\n",
      "\n",
      "ğŸ¤– Os documentos fornecidos parecem ser uma coleÃ§Ã£o de textos relacionados a diferentes tÃ³picos, incluindo:\n",
      "\n",
      "1. Um guia prÃ¡tico de SQL (Structured Query Language) para aprender e trabalhar com banco de dados.\n",
      "2. Um artigo sobre a arquitetura de rede neural chamada \"Attention is All You Need\" (AtenÃ§Ã£o Ã© tudo o que vocÃª precisa), que foi apresentado em uma conferÃªncia de inteligÃªncia artificial.\n",
      "3. RelatÃ³rios de empresa sobre a Bradesco, uma instituiÃ§Ã£o financeira brasileira, que incluem informaÃ§Ãµes sobre a estratÃ©gia de negÃ³cios, produtos e serviÃ§os oferecidos pela empresa.\n",
      "4. Outros textos que nÃ£o foram identificados com certeza, mas que parecem ser relacionados a tecnologia e negÃ³cios.\n",
      "\n",
      "Em resumo, os documentos parecem ser uma mistura de conteÃºdo tÃ©cnico e empresarial.\n",
      "\n",
      "ğŸ“š Fontes: 12\n",
      "   ğŸ“„ O'Reilly - PT - SQL Guia PraÌtico - Alice Zhao.pdf (pÃ¡gs: 25, 178, 17)\n",
      "   ğŸ“„ NIPS-2017-attention-is-all-you-need-Paper.pdf (pÃ¡gs: 9, 8)\n",
      "\n",
      "â±ï¸  1.31s\n"
     ]
    }
   ],
   "source": [
    "bot.chat('Sobre o que sÃ£o estes documentos?');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfee7ba-ad09-439e-b08c-3cbdafe4f94d",
   "metadata": {},
   "source": [
    "#### ALTERANDO MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8eb632dd-07fd-4e2d-a6b1-4893ce697d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': True,\n",
       " 'message': 'Nenhuma alteraÃ§Ã£o',\n",
       " 'changes': [],\n",
       " 'recreated': [],\n",
       " 'current_config': {'model': 'openai/gpt-oss-20b',\n",
       "  'temperature': 0.1,\n",
       "  'max_tokens': 2048,\n",
       "  'chunk_size': 1000,\n",
       "  'chunk_overlap': 200,\n",
       "  'embeddings_model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       "  'retriever_k': 4,\n",
       "  'retriever_fetch_k': 8,\n",
       "  'max_history_messages': 10,\n",
       "  'index_dir': 'indices'},\n",
       " 'warning': None}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.update_config(model=GROQ_MODELS['gpt-oss-20b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "91f8edb5-5214-4bee-ae3e-b5f1fecb4a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents_in_memory': 3,\n",
       " 'document_names': ['O_Reilly_-_PT_-_SQL_Guia_Pra_tico_-_Alice_Zhao',\n",
       "  'NIPS-2017-attention-is-all-you-need-Paper',\n",
       "  '000906000101011'],\n",
       " 'history_messages': 7,\n",
       " 'model': 'openai/gpt-oss-20b',\n",
       " 'temperature': 0.1,\n",
       " 'embeddings': 'sentence-transformers/all-MiniLM-L6-v2'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.get_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150e9de8-415b-42e1-8b7f-e7c13b17e943",
   "metadata": {},
   "source": [
    "#### CHAT COM NOVO MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1a6db4a0-42fd-4ec2-8efb-03d4fee50f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘¤ Resuma os temas mais relavantes do bradesco. \n",
      "ğŸ” Buscando em 3 documento(s)...\n",
      "\n",
      "ğŸ¤– Com base nos documentos fornecidos, os temas mais relevantes do Bradesco sÃ£o:\n",
      "\n",
      "1. **ExpansÃ£o de produtos e serviÃ§os**: O Bradesco estÃ¡ expandindo sua oferta de produtos e serviÃ§os, incluindo financiamentos, seguros, consÃ³rcios e soluÃ§Ãµes de pagamento.\n",
      "2. **InovaÃ§Ã£o e tecnologia**: A empresa estÃ¡ investindo em tecnologia para melhorar a eficiÃªncia e a experiÃªncia do cliente, incluindo a implementaÃ§Ã£o do Salesforce e a criaÃ§Ã£o de um portal de desenvolvedores.\n",
      "3. **Crescimento no mercado**: O Bradesco estÃ¡ crescendo no mercado, com um aumento de 2,3% no market share de seguros e um aumento de 3,5% no market share de DCM (Debt Capital Markets) em 2025.\n",
      "4. **ExpansÃ£o de canais de distribuiÃ§Ã£o**: A empresa estÃ¡ expandindo seus canais de distribuiÃ§Ã£o, incluindo a criaÃ§Ã£o de um novo canal transacional web para pessoas jurÃ­dicas e a expansÃ£o de sua presenÃ§a em plataformas de pagamento.\n",
      "5. **Foco em SMEs**: O Bradesco estÃ¡ focando em atender Ã s necessidades dos pequenos e mÃ©dios empresÃ¡rios (SMEs), oferecendo soluÃ§Ãµes de financiamento, seguros e outros produtos e serviÃ§os personalizados para essa categoria de clientes.\n",
      "\n",
      "ğŸ“š Fontes: 12\n",
      "   ğŸ“„ O'Reilly - PT - SQL Guia PraÌtico - Alice Zhao.pdf (pÃ¡gs: 300, 13, 266)\n",
      "   ğŸ“„ NIPS-2017-attention-is-all-you-need-Paper.pdf (pÃ¡gs: 8, 1)\n",
      "\n",
      "â±ï¸  2.59s\n"
     ]
    }
   ],
   "source": [
    "bot.chat('Resuma os temas mais relavantes do bradesco. ');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a825e86f-aa0c-4fe4-adf7-551a279add4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘¤ resuma o mecanismo de atenÃ§Ã£o em emojis.\n",
      "ğŸ” Buscando em 3 documento(s)...\n",
      "\n",
      "ğŸ¤– ğŸ¤”ğŸ’¡ğŸ“ğŸ‘€ğŸ’¬ğŸ”ï¸ğŸ’»ï¸\n",
      "\n",
      "Essas emojis resumem o mecanismo de atenÃ§Ã£o, que Ã© um componente fundamental da rede neural chamada \"Attention is All You Need\". Aqui estÃ¡ uma explicaÃ§Ã£o de cada emoji:\n",
      "\n",
      "ğŸ¤”: Pensamento e processamento de informaÃ§Ãµes\n",
      "ğŸ’¡: IluminaÃ§Ã£o e compreensÃ£o da informaÃ§Ã£o\n",
      "ğŸ“: Processamento de linguagem natural\n",
      "ğŸ‘€: Foco e atenÃ§Ã£o\n",
      "ğŸ’¬: ComunicaÃ§Ã£o e interaÃ§Ã£o entre as partes da rede neural\n",
      "ğŸ”ï¸: Busca e seleÃ§Ã£o de informaÃ§Ãµes relevantes\n",
      "ğŸ’»ï¸: Processamento e armazenamento de informaÃ§Ãµes\n",
      "\n",
      "ğŸ“š Fontes: 12\n",
      "   ğŸ“„ O'Reilly - PT - SQL Guia PraÌtico - Alice Zhao.pdf (pÃ¡gs: 157, 136, 183)\n",
      "   ğŸ“„ NIPS-2017-attention-is-all-you-need-Paper.pdf (pÃ¡gs: 1, 10)\n",
      "\n",
      "â±ï¸  1.87s\n"
     ]
    }
   ],
   "source": [
    "bot.chat('resuma o mecanismo de atenÃ§Ã£o em emojis.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f3302288-229e-4b4e-959d-b4fb2911264a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘¤ me mostre um exemplo de query sql.\n",
      "ğŸ” Buscando em 3 documento(s)...\n",
      "\n",
      "ğŸ¤– Aqui estÃ¡ um exemplo de query SQL simples:\n",
      "\n",
      "```sql\n",
      "SELECT nome, idade\n",
      "FROM clientes\n",
      "WHERE cidade = 'SÃ£o Paulo';\n",
      "```\n",
      "\n",
      "Essa query faz o seguinte:\n",
      "\n",
      "* `SELECT nome, idade`: Seleciona as colunas `nome` e `idade` da tabela `clientes`.\n",
      "* `FROM clientes`: Indica a tabela que serÃ¡ consultada, que Ã© a tabela `clientes`.\n",
      "* `WHERE cidade = 'SÃ£o Paulo'`: Filtro que seleciona apenas os registros onde a coluna `cidade` tem o valor `'SÃ£o Paulo'`.\n",
      "\n",
      "Essa query retornarÃ¡ uma lista com os nomes e idades dos clientes que residem em SÃ£o Paulo.\n",
      "\n",
      "ğŸ“š Fontes: 12\n",
      "   ğŸ“„ O'Reilly - PT - SQL Guia PraÌtico - Alice Zhao.pdf (pÃ¡gs: 59, 1, 48)\n",
      "   ğŸ“„ NIPS-2017-attention-is-all-you-need-Paper.pdf (pÃ¡gs: 8, 2)\n",
      "\n",
      "â±ï¸  1.47s\n"
     ]
    }
   ],
   "source": [
    "bot.chat('me mostre um exemplo de query sql.');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac10b9-3d64-45f5-8b0d-7f5516ee3fcb",
   "metadata": {},
   "source": [
    "#### UTILITARIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "07be8155-a593-4554-a1e6-750f4f787629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents_in_memory': 3,\n",
       " 'document_names': ['O_Reilly_-_PT_-_SQL_Guia_Pra_tico_-_Alice_Zhao',\n",
       "  'NIPS-2017-attention-is-all-you-need-Paper',\n",
       "  '000906000101011'],\n",
       " 'history_messages': 13,\n",
       " 'model': 'openai/gpt-oss-20b',\n",
       " 'temperature': 0.1,\n",
       " 'embeddings': 'sentence-transformers/all-MiniLM-L6-v2'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "16b84667-d73f-4416-b169-0f20dcf71284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'openai/gpt-oss-20b',\n",
       " 'temperature': 0.1,\n",
       " 'max_tokens': 2048,\n",
       " 'chunk_size': 1000,\n",
       " 'chunk_overlap': 200,\n",
       " 'embeddings_model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       " 'retriever_k': 4,\n",
       " 'retriever_fetch_k': 8,\n",
       " 'max_history_messages': 10,\n",
       " 'index_dir': 'indices'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f191e165-dc1d-4071-af99-b096ca8c9310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'O_Reilly_-_PT_-_SQL_Guia_Pra_tico_-_Alice_Zhao',\n",
       "  'original_file': \"C:\\\\Users\\\\wsant\\\\Downloads\\\\O'Reilly - PT - SQL Guia PraÌtico - Alice Zhao.pdf\",\n",
       "  'chunks': 537,\n",
       "  'pages': 306,\n",
       "  'created': '2026-02-09T00:26:28.974202'},\n",
       " {'name': 'NIPS-2017-attention-is-all-you-need-Paper',\n",
       "  'original_file': 'C:\\\\Users\\\\wsant\\\\Downloads\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
       "  'chunks': 43,\n",
       "  'pages': 11,\n",
       "  'created': '2026-02-09T00:27:50.841445'},\n",
       " {'name': '000906000101011',\n",
       "  'original_file': 'C:\\\\Users\\\\wsant\\\\Downloads\\\\000906000101011.pdf',\n",
       "  'chunks': 30,\n",
       "  'pages': 25,\n",
       "  'created': '2026-02-09T00:27:51.908349'}]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.list_loaded_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3eaca7ae-92b5-464f-a809-ff5ca8f08baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Recriando retrievers...\n",
      "ğŸ”„ Recriando RAG chain...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'success': True,\n",
       " 'message': 'Atualizado (2 mudanÃ§as)',\n",
       " 'changes': ['retriever_k: 4 â†’ 3', 'retriever_fetch_k: 8 â†’ 5'],\n",
       " 'recreated': ['retrievers', 'rag_chain'],\n",
       " 'current_config': {'model': 'openai/gpt-oss-20b',\n",
       "  'temperature': 0.1,\n",
       "  'max_tokens': 2048,\n",
       "  'chunk_size': 1000,\n",
       "  'chunk_overlap': 200,\n",
       "  'embeddings_model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       "  'retriever_k': 3,\n",
       "  'retriever_fetch_k': 5,\n",
       "  'max_history_messages': 10,\n",
       "  'index_dir': 'indices'},\n",
       " 'warning': None}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.update_config(retriever_k=3, retriever_fetch_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1a24a06d-7e69-44ea-a93f-b229860ebae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'openai/gpt-oss-20b',\n",
       " 'temperature': 0.1,\n",
       " 'max_tokens': 2048,\n",
       " 'chunk_size': 1000,\n",
       " 'chunk_overlap': 200,\n",
       " 'embeddings_model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       " 'retriever_k': 3,\n",
       " 'retriever_fetch_k': 5,\n",
       " 'max_history_messages': 10,\n",
       " 'index_dir': 'indices'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f3d531d1-d525-4960-bf07-6fe3bf4ae689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘¤ o que Ã© o foco em SMEs?\n",
      "ğŸ” Buscando em 3 documento(s)...\n",
      "\n",
      "ğŸ¤– O foco em SMEs (Pequenas e MÃ©dias Empresas) Ã© uma estratÃ©gia de negÃ³cios que visa atender Ã s necessidades especÃ­ficas dessas empresas.\n",
      "\n",
      "SMEs sÃ£o empresas que tÃªm um nÃºmero limitado de funcionÃ¡rios e um volume de negÃ³cios menor em comparaÃ§Ã£o com as grandes empresas. Elas precisam de soluÃ§Ãµes de financiamento, seguros, tecnologia e outros produtos e serviÃ§os que sejam personalizados para suas necessidades especÃ­ficas.\n",
      "\n",
      "O foco em SMEs pode incluir:\n",
      "\n",
      "* Oferecer soluÃ§Ãµes de financiamento personalizadas para ajudar as SMEs a crescerem e se desenvolverem.\n",
      "* Desenvolver produtos e serviÃ§os de seguros que sejam adequados Ã s necessidades das SMEs.\n",
      "* Criar plataformas de tecnologia que sejam fÃ¡ceis de usar e acessÃ­veis para as SMEs.\n",
      "* Oferecer consultoria e apoio para ajudar as SMEs a melhorar sua gestÃ£o e produtividade.\n",
      "\n",
      "O objetivo do foco em SMEs Ã© ajudar essas empresas a crescerem e se desenvolverem, criando empregos e contribuindo para a economia local.\n",
      "\n",
      "No caso do Bradesco, o foco em SMEs pode incluir:\n",
      "\n",
      "* Oferecer soluÃ§Ãµes de financiamento para ajudar as SMEs a crescerem e se desenvolverem.\n",
      "* Desenvolver produtos e serviÃ§os de seguros que sejam adequados Ã s necessidades das SMEs.\n",
      "* Criar plataformas de tecnologia que sejam fÃ¡ceis de usar e acessÃ­veis para as SMEs.\n",
      "* Oferecer consultoria e apoio para ajudar as SMEs a melhorar sua gestÃ£o e produtividade.\n",
      "\n",
      "ğŸ“š Fontes: 9\n",
      "   ğŸ“„ O'Reilly - PT - SQL Guia PraÌtico - Alice Zhao.pdf (pÃ¡gs: 130, 24, 14)\n",
      "   ğŸ“„ NIPS-2017-attention-is-all-you-need-Paper.pdf (pÃ¡gs: 3, 10, 5)\n",
      "\n",
      "â±ï¸  2.12s\n"
     ]
    }
   ],
   "source": [
    "bot.chat('o que Ã© o foco em SMEs?');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
