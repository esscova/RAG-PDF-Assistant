{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "905cc01b-7edb-41fc-a71e-8194c8193380",
   "metadata": {},
   "source": [
    "# ChatBot RAG com documentos PDF\n",
    "---\n",
    "Este notebook combina persistencia automatica + contextualiza√ß√£o LCEL com recursos de m√∫ltiplos pdfs e merge de indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d115cc-453c-4ab3-b75f-22467f097548",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a9b4c9-45b0-48a8-9b2e-827275d55a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, Any, List, Optional, Union\n",
    "\n",
    "# LLM \n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "## core\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableBranch\n",
    "\n",
    "# pdf loader e vectordb\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embeddings e textsplitters\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5080a9b-a2ba-467d-abff-7e6ff9404f90",
   "metadata": {},
   "source": [
    "### CONFIGURA√á√ÉO DA API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39f66fb9-0360-4919-a6f6-562d7b48963e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key configurada!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "\n",
    "if not GROQ_API_KEY:\n",
    "    raise ValueError(\"GROQ_API_KEY n√£o foi configurada!\")\n",
    "\n",
    "print(\"API Key configurada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe165f46-9132-4d43-b60e-468889f65e44",
   "metadata": {},
   "source": [
    "### CONFIGURA√á√ïES GLOBAIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07d06873-4c4d-4b2e-b355-a233626209a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_MODELS = {\n",
    "    \"llama-3.3-70b\": \"llama-3.3-70b-versatile\",\n",
    "    \"llama-3.1-8b\": \"llama-3.1-8b-instant\",\n",
    "    \"kimi-k2\": \"moonshotai/kimi-k2-instruct\",\n",
    "    \"gpt-oss-20b\": \"openai/gpt-oss-20b\",\n",
    "    \"qwen3-32b\": \"qwen/qwen3-32b\"\n",
    "}\n",
    "\n",
    "DEFAULT_CONFIG = {\n",
    "    \"model\": GROQ_MODELS[\"llama-3.1-8b\"],\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 2048,\n",
    "    \"chunk_size\": 1000,\n",
    "    \"chunk_overlap\": 200,\n",
    "    \"embeddings_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"retriever_k\": 4,\n",
    "    \"retriever_fetch_k\": 8,\n",
    "    \"max_history_messages\": 10,\n",
    "    \"index_path\": \"faiss_index\",  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d97087a-81c1-4a3a-92e8-1bce85e946d9",
   "metadata": {},
   "source": [
    "### CLASSE PRINCIPAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9acef7df-38fc-4fd9-b000-139e38168d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGChatBot:\n",
    "    \"\"\"\n",
    "    ChatBot RAG com persist√™ncia, contextualiza√ß√£o e modo h√≠brido.\n",
    "\n",
    "    Uso:\n",
    "        bot = RAGChatBot()\n",
    "        bot.load_documents(['doc1.pdf', 'doc2.pdf'])\n",
    "        bot.chat('Qual o tema principal?')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config:Dict[str,Any]=None, api_key:str=None):\n",
    "        \"\"\"\n",
    "        Inicializa o chatbot\n",
    "\n",
    "        Args:\n",
    "            config: configura√ß√µes personalizadas (opcional)\n",
    "            api_key: chave API Groq(opcional, l√™ do .env se n√£o fornecida)\n",
    "        \"\"\"\n",
    "        self.config = config or DEFAULT_CONFIG.copy()\n",
    "        self.api_key = api_key or os.getenv('GROQ_API_KEY')\n",
    "\n",
    "        if not self.api_key:\n",
    "            raise ValueError('GROQ_API_KEY n√£o configurada, use .env ou passe api_key=')\n",
    "\n",
    "        self.llm = None\n",
    "        self.history:List[Any] = []\n",
    "        self.documents_loaded = False\n",
    "        self.vectorstore = None\n",
    "        self.embeddings = None\n",
    "        self.retriever = None\n",
    "        self.rag_chain = None\n",
    "        self.contextualized_retriever = None\n",
    "\n",
    "\n",
    "        print('=== INICIALIZANDO CHATBOT ===')\n",
    "        self._initialize_llm()\n",
    "        self._initialize_system_prompt()\n",
    "        self._initialize_embeddings()\n",
    "        print('ChatBot pronto!\\n')\n",
    "\n",
    "    def _initialize_llm(self):\n",
    "        \"\"\"Inicializa o modelo de linguagem Groq\"\"\"\n",
    "        self.llm = ChatGroq(\n",
    "            model=self.config['model'],\n",
    "            temperature=self.config['temperature'],\n",
    "            max_tokens=self.config['max_tokens'],\n",
    "            api_key=self.api_key\n",
    "        )\n",
    "        print(f'LLM: {self.config[\"model\"]} carregado!')\n",
    "\n",
    "    def _initialize_system_prompt(self):\n",
    "        \"\"\"Mensagem fixa do sistema\"\"\"\n",
    "        system_message = SystemMessage(\n",
    "            content=\"\"\"Voc√™ √© um assistente prestativo especializado em responder perguntas com base em documentos fornecidos. \n",
    "Seja objetivo, preciso e cite fontes quando poss√≠vel.\"\"\")\n",
    "        self.history.append(system_message)\n",
    "\n",
    "    def _initialize_embeddings(self):\n",
    "        \"\"\"Inicializa o modelo de embeedings\"\"\"\n",
    "        print(f\"Carregando embeddings: {self.config['embeddings_model']}...\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=self.config['embeddings_model'],\n",
    "            model_kwargs={'device':'cpu'},\n",
    "            encode_kwargs={'normalize_embeddings':True},\n",
    "        )\n",
    "        print('Embeddings carregados!')\n",
    "    \n",
    "    def load_documents(\n",
    "            self, \n",
    "            pdf_paths: Union[str, List[str]], \n",
    "            index_path: str = None,\n",
    "            force_recreate: bool = False\n",
    "        ) -> bool:\n",
    "            \"\"\"\n",
    "            Carrega PDFs criando ou carregando √≠ndice FAISS.\n",
    "    \n",
    "            Args:\n",
    "                pdf_paths: Caminho ou lista de caminhos de PDFs\n",
    "                index_path: Pasta para salvar/carregar √≠ndice (padr√£o: config['index_path'])\n",
    "                force_recreate: Se True, recria √≠ndice mesmo se existir\n",
    "    \n",
    "            Returns:\n",
    "                True se sucesso, False caso contr√°rio\n",
    "            \"\"\"\n",
    "            if isinstance(pdf_paths, str):\n",
    "                 pdf_paths = [pdf_paths]\n",
    "    \n",
    "            index_path = index_path or self.config['index_path']\n",
    "    \n",
    "            print('Carregando {} documento(s)'.format(len(pdf_paths)))\n",
    "    \n",
    "            try:\n",
    "                if not force_recreate and os.path.exists(index_path) and os.path.isdir(index_path): #indice salvo?\n",
    "                    print('Indice encontrado em {}'.format(index_path))\n",
    "                    self._load_vectorstore(index_path)\n",
    "                    print('Indice carregado do disco.')\n",
    "                else:\n",
    "                    if force_recreate and os.path.exists(index_path):\n",
    "                        print('Removendo √≠ndice antigo...')\n",
    "                        shutil.rmtree(index_path)\n",
    "    \n",
    "                    all_chunks = self._process_pdfs(pdf_paths)\n",
    "                    if not all_chunks:\n",
    "                        print('Nenhum documento processado.')\n",
    "                        return False\n",
    "    \n",
    "                    print('Criando indice FAISS com {} chunks.'.format(len(all_chunks)))\n",
    "                    self.vectorstore = FAISS.from_documents(all_chunks, self.embeddings)\n",
    "                    self.vectorstore.save_local(index_path)\n",
    "                    print('Indice salvo em {}'.format(index_path))\n",
    "    \n",
    "                self._setup_retriever()\n",
    "                self._create_rag_chain()\n",
    "                self.documents_loaded = True\n",
    "    \n",
    "                print('Sistema RAG ativo.')\n",
    "                return True\n",
    "    \n",
    "            except Exception as e:\n",
    "                print('Erro: {}'.format( str(e) ))\n",
    "                return False            \n",
    "\n",
    "    def _process_pdfs(self, pdf_paths:List[str]) -> List[Document]:\n",
    "        \"\"\"Processa pdfs e retorna chunks\"\"\"\n",
    "        all_chunks = []\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.config[\"chunk_size\"],\n",
    "            chunk_overlap=self.config[\"chunk_overlap\"],\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \";\", \" \", \"\"]\n",
    "        )\n",
    "\n",
    "        for pdf in pdf_paths:\n",
    "            if not os.path.exists(pdf):\n",
    "                print('arquivo n√£o encontrado: {}'.format(pdf))\n",
    "                continue\n",
    "\n",
    "            print('Processando: {}'.format( Path(pdf).name ))\n",
    "            loader = PyMuPDFLoader(pdf)\n",
    "            docs = loader.load()\n",
    "\n",
    "            chunks = text_splitter.split_documents(docs)\n",
    "            all_chunks.extend(chunks)\n",
    "            print('{} paginas -> {} chunks'.format( len(docs), len(chunks) ))\n",
    "\n",
    "        return all_chunks\n",
    "\n",
    "    def _load_vectorstore(self, path:str):\n",
    "        \"\"\"Carrega √≠ndice FAISS do disco\"\"\"\n",
    "        self.vectorstore = FAISS.load_local(\n",
    "            path,\n",
    "            self.embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "\n",
    "    def _setup_retriever(self):\n",
    "        \"\"\"Configura retriever com MMR\"\"\"\n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type='mmr',\n",
    "            search_kwargs={\n",
    "                'k': self.config[\"retriever_k\"],\n",
    "                \"fetch_k\": self.config[\"retriever_fetch_k\"],\n",
    "                \"lambda_mult\": 0.7\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def _create_rag_chain(self):\n",
    "        \"\"\"Cria chain RAG com contextualiza√ß√£o de hist√≥rico\"\"\"\n",
    "        \n",
    "        contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Dado o hist√≥rico de conversa√ß√£o e a pergunta atual do usu√°rio,\n",
    "                          reformule a pergunta para que seja independente do contexto anterior.\n",
    "                          Mantenha o idioma original (portugu√™s).\n",
    "                          N√ÉO responda a pergunta, apenas reformule-a se necess√°rio.\"\"\"),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ])\n",
    "\n",
    "        contextualize_chain = contextualize_q_prompt | self.llm | StrOutputParser()\n",
    "\n",
    "        def contextualize_or_not(x: dict) -> str:\n",
    "            \"\"\"Reformula se houver hist√≥rico, sen√£o usa pergunta original.\"\"\"\n",
    "            if x.get(\"chat_history\") and len(x[\"chat_history\"]) > 1: # tem historico com 2 msgs?\n",
    "                return contextualize_chain.invoke(x)\n",
    "            return x[\"input\"]\n",
    "\n",
    "        # retriever contextualizado\n",
    "        self.contextualized_retriever = (\n",
    "            RunnablePassthrough.assign(\n",
    "                reformulated_question=lambda x: contextualize_or_not(x)\n",
    "            )\n",
    "            | (lambda x: self.retriever.invoke(x[\"reformulated_question\"]))\n",
    "        )\n",
    "\n",
    "        # chain RAG completa\n",
    "        qa_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Voc√™ √© um assistente especializado em responder perguntas baseadas em documentos.\n",
    "\n",
    "                            INSTRU√á√ïES:\n",
    "                            - Use APENAS o contexto fornecido abaixo para responder\n",
    "                            - Se n√£o souber a resposta, diga \"N√£o encontrei essa informa√ß√£o nos documentos\"\n",
    "                            - Cite a fonte (p√°gina) quando poss√≠vel\n",
    "                            - Seja conciso e objetivo\n",
    "                            - Responda em portugu√™s\n",
    "                            \n",
    "                            Contexto:\n",
    "                            {context}\"\"\"),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ])\n",
    "\n",
    "        def format_docs(docs: List[Document]) -> str:\n",
    "            \"\"\"Formata documentos para string.\"\"\"\n",
    "            formatted = []\n",
    "            for i, doc in enumerate(docs, 1):\n",
    "                page = doc.metadata.get(\"page\", \"?\")\n",
    "                source = Path(doc.metadata.get(\"source\", \"\")).name\n",
    "                formatted.append(f\"[Doc {i} | P√°g. {page} | {source}]\\n{doc.page_content}\")\n",
    "            return \"\\n\\n---\\n\\n\".join(formatted)\n",
    "\n",
    "        # chain\n",
    "        self.rag_chain = (\n",
    "            RunnablePassthrough.assign(\n",
    "                context=lambda x: format_docs(\n",
    "                    self.contextualized_retriever.invoke(x)\n",
    "                )\n",
    "            )\n",
    "            | qa_prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def chat(\n",
    "        self, \n",
    "        user_input: str, \n",
    "        use_rag: bool = True, \n",
    "        verbose: bool = True\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Processa pergunta do usu√°rio.\n",
    "\n",
    "        Args:\n",
    "            user_input: Pergunta\n",
    "            use_rag: Se True, usa documentos; se False, chat puro\n",
    "            verbose: Se True, mostra detalhes\n",
    "\n",
    "        Returns:\n",
    "            Dict com answer, sources (se RAG), time, etc.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nüë§ Voc√™: {user_input}\")\n",
    "\n",
    "        # modo RAG\n",
    "        if use_rag and self.documents_loaded:\n",
    "            if verbose:\n",
    "                print(\"üîç Buscando contexto...\")\n",
    "            \n",
    "            response = self.rag_chain.invoke({\n",
    "                \"input\": user_input,\n",
    "                \"chat_history\": self.history\n",
    "            })\n",
    "\n",
    "            # sources para metadados\n",
    "            sources = self.contextualized_retriever.invoke({\n",
    "                \"input\": user_input,\n",
    "                \"chat_history\": self.history\n",
    "            })\n",
    "\n",
    "            # apendar historico\n",
    "            self.history.append(HumanMessage(content=user_input))\n",
    "            self.history.append(AIMessage(content=response))\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\nü§ñ Assistente: {response}\")\n",
    "                print(f\"\\nüìö Fontes consultadas: {len(sources)}\")\n",
    "                for i, doc in enumerate(sources[:3], 1):  # top 3\n",
    "                    page = doc.metadata.get(\"page\", \"?\")\n",
    "                    source = Path(doc.metadata.get(\"source\", \"\")).name\n",
    "                    print(f\"   [{i}] {source} - p√°g. {page}\")\n",
    "                print(f\"\\n‚è±Ô∏è  Tempo: {elapsed:.2f}s\")\n",
    "\n",
    "            # limit hist√≥rico\n",
    "            self._trim_history()\n",
    "\n",
    "            return {\n",
    "                \"answer\": response,\n",
    "                \"sources\": sources,\n",
    "                \"time\": elapsed,\n",
    "                \"mode\": \"RAG\"\n",
    "            }\n",
    "\n",
    "        # modo chat\n",
    "        else:\n",
    "            if use_rag and not self.documents_loaded:\n",
    "                if verbose:\n",
    "                    print(\"RAG solicitado mas documentos n√£o carregados. Usando modo chat.\")\n",
    "\n",
    "            # gerar e apendar\n",
    "            self.history.append(HumanMessage(content=user_input))\n",
    "            response = self.llm.invoke(self.history)\n",
    "            self.history.append(AIMessage(content=response.content))\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\nü§ñ Assistente: {response.content}\")\n",
    "                print(f\"\\n‚è±Ô∏è  Tempo: {elapsed:.2f}s (modo chat)\")\n",
    "\n",
    "            self._trim_history()\n",
    "\n",
    "            return {\n",
    "                \"answer\": response.content,\n",
    "                \"sources\": [],\n",
    "                \"time\": elapsed,\n",
    "                \"mode\": \"CHAT\"\n",
    "            }\n",
    "\n",
    "    def _trim_history(self):\n",
    "        \"\"\"Limita tamanho do hist√≥rico mantendo system message.\"\"\"\n",
    "        max_msgs = self.config[\"max_history_messages\"] * 2  # Human + AI\n",
    "        if len(self.history) > max_msgs + 1:  # +1 para system message\n",
    "            # manter system message e √∫ltimas mensagens\n",
    "            self.history = [self.history[0]] + self.history[-max_msgs:]\n",
    "\n",
    "\n",
    "    def clear_history(self):\n",
    "        \"\"\"Limpa hist√≥rico mantendo system message.\"\"\"\n",
    "        system_msg = self.history[0] if self.history else None\n",
    "        self.history = [system_msg] if system_msg else []\n",
    "        print(\"Hist√≥rico limpo!\")\n",
    "\n",
    "    def clear_documents(self):\n",
    "        \"\"\"Remove documentos carregados.\"\"\"\n",
    "        self.vectorstore = None\n",
    "        self.retriever = None\n",
    "        self.rag_chain = None\n",
    "        self.contextualized_retriever = None\n",
    "        self.documents_loaded = False\n",
    "        print(\"Documentos removidos!\")\n",
    "\n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Retorna estat√≠sticas do sistema.\"\"\"\n",
    "        stats = {\n",
    "            \"documents_loaded\": self.documents_loaded,\n",
    "            \"history_messages\": len(self.history),\n",
    "            \"model\": self.config[\"model\"],\n",
    "            \"embeddings\": self.config[\"embeddings_model\"],\n",
    "        }\n",
    "\n",
    "        if self.vectorstore:\n",
    "            stats[\"vectors_in_index\"] = self.vectorstore.index.ntotal\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def save_index(self, path: str = None):\n",
    "        \"\"\"Salva √≠ndice FAISS manualmente.\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            print(\"Nenhum √≠ndice para salvar!\")\n",
    "            return\n",
    "\n",
    "        path = path or self.config[\"index_path\"]\n",
    "        self.vectorstore.save_local(path)\n",
    "        print(f\"√çndice salvo em: {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866f9d4e-ee96-47d8-b37a-453f4785631c",
   "metadata": {},
   "source": [
    "### INICIALIZAR CHATBOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c0518d7-c63d-44ca-befb-a2ff143cb49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIALIZANDO CHATBOT ===\n",
      "LLM: llama-3.1-8b-instant carregado!\n",
      "Carregando embeddings: sentence-transformers/all-MiniLM-L6-v2...\n",
      "Embeddings carregados!\n",
      "ChatBot pronto!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bot = RAGChatBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a1801ff-4347-4df0-aa02-e77a2066a0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents_loaded': False,\n",
       " 'history_messages': 1,\n",
       " 'model': 'llama-3.1-8b-instant',\n",
       " 'embeddings': 'sentence-transformers/all-MiniLM-L6-v2'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.get_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a658f091-effd-4d22-a1ff-128352526446",
   "metadata": {},
   "source": [
    "### FAZER PERGUNTAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bad90c-867e-4e12-af0f-94aa8f49187c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### CHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb74eee2-77d4-43ee-8def-6979b99c6a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ Voc√™: Qual a capital da Fran√ßa\n",
      "RAG solicitado mas documentos n√£o carregados. Usando modo chat.\n",
      "\n",
      "ü§ñ Assistente: A capital da Fran√ßa √© Paris. De acordo com a Constitui√ß√£o da Fran√ßa de 1958, Paris √© a capital do pa√≠s e sede do governo. Al√©m disso, a cidade de Paris √© tamb√©m a maior cidade da Fran√ßa e um dos principais centros culturais, econ√¥micos e pol√≠ticos do mundo.\n",
      "\n",
      "Fonte: Constitui√ß√£o da Fran√ßa de 1958, artigo 2.\n",
      "\n",
      "Refer√™ncia: \"Constitui√ß√£o da Fran√ßa\" (2020). Dispon√≠vel em: <https://www.legifrance.gouv.fr/affichTexte.do?cidTexte=JORFTEXT000000 000000&categorieLien=id>\n",
      "\n",
      "√â importante notar que a capital da Fran√ßa pode ser alterada em algum momento no futuro, mas, atualmente, Paris √© a capital do pa√≠s.\n",
      "\n",
      "‚è±Ô∏è  Tempo: 0.51s (modo chat)\n"
     ]
    }
   ],
   "source": [
    "bot.chat('Qual a capital da Fran√ßa');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8af87810-b67f-4303-b2af-2025a7c84f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ Voc√™: Resuma econometria em uma frase curta e provocante.\n",
      "\n",
      "ü§ñ Assistente: \"A econometria √© a arte de prever o futuro com base no passado, mas com a certeza de que o futuro sempre vai surpreender!\"\n",
      "\n",
      "‚è±Ô∏è  Tempo: 0.41s (modo chat)\n"
     ]
    }
   ],
   "source": [
    "bot.chat('Resuma econometria em uma frase curta e provocante.', use_rag=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a336456-1db0-413c-9a1c-39af3d7f908a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'ü§ñ aprende üìä dados üîç analisa üí° prediz',\n",
       " 'sources': [],\n",
       " 'time': 0.2793867588043213,\n",
       " 'mode': 'CHAT'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.chat('Explique machine learning em uma linha e somente com emojis',verbose=False, use_rag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3aac7114-f46c-4184-b25e-d0a35af2a573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ Voc√™: qual foi minha primeira pergunta?\n",
      "\n",
      "ü§ñ Assistente: Sua primeira pergunta foi: \"Qual a capital da Fran√ßa\"\n",
      "\n",
      "‚è±Ô∏è  Tempo: 0.56s (modo chat)\n"
     ]
    }
   ],
   "source": [
    "bot.chat('qual foi minha primeira pergunta?', use_rag=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aaf0a3c9-2450-4fc3-a404-000a689e0947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist√≥rico limpo!\n"
     ]
    }
   ],
   "source": [
    "bot.clear_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ddb3d1-aab5-4813-af52-51f421f5589d",
   "metadata": {},
   "source": [
    "#### CHAT COM RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "581d48ba-6491-4fcf-bf33-e44250903e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIALIZANDO CHATBOT ===\n",
      "LLM: llama-3.1-8b-instant carregado!\n",
      "Carregando embeddings: sentence-transformers/all-MiniLM-L6-v2...\n",
      "Embeddings carregados!\n",
      "ChatBot pronto!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bot_rag = RAGChatBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d345d8e5-97b8-461a-a648-59632d8ba235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents_loaded': False,\n",
       " 'history_messages': 1,\n",
       " 'model': 'llama-3.1-8b-instant',\n",
       " 'embeddings': 'sentence-transformers/all-MiniLM-L6-v2'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot_rag.get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ac1320e4-500e-4c53-8f4b-ccd7ad9dfe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando 1 documento(s)\n",
      "Indice encontrado em faiss_index\n",
      "Indice carregado do disco.\n",
      "Sistema RAG ativo.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# carregar pdf\n",
    "bot_rag.load_documents(\"C:\\\\Users\\\\wsant\\\\Downloads\\\\O'Reilly - PT - SQL Guia PraÃÅtico - Alice Zhao.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6e56363a-c1f2-4411-8e9f-6377c6b71cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ Voc√™: Qual o principal tema deste documento?\n",
      "üîç Buscando contexto...\n",
      "\n",
      "ü§ñ Assistente: O principal tema deste documento √© o SQL (Structured Query Language) e a sua aplica√ß√£o em bancos de dados relacionais.\n",
      "\n",
      "üìö Fontes consultadas: 4\n",
      "   [1] O'Reilly - PT - SQL Guia PraÃÅtico - Alice Zhao.pdf - p√°g. 13\n",
      "   [2] O'Reilly - PT - SQL Guia PraÃÅtico - Alice Zhao.pdf - p√°g. 17\n",
      "   [3] O'Reilly - PT - SQL Guia PraÃÅtico - Alice Zhao.pdf - p√°g. 295\n",
      "\n",
      "‚è±Ô∏è  Tempo: 0.52s\n"
     ]
    }
   ],
   "source": [
    "bot_rag.chat('Qual o principal tema deste documento?');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac3e7914-1731-44f3-ad6b-9512b7cb175d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ Voc√™: Quais s√£o as principais conclus√µes ou recomenda√ß√µes?\n",
      "üîç Buscando contexto...\n",
      "\n",
      "ü§ñ Assistente: As principais conclus√µes ou recomenda√ß√µes deste documento s√£o:\n",
      "\n",
      "1. N√£o h√° uma resposta definitiva para o tipo de dado ideal para uma coluna, pois depende do espa√ßo de armazenamento e da flexibilidade requeridos (Doc 1, P√°g. 145).\n",
      "2. √â recomend√°vel criar √≠ndices em colunas que s√£o usadas com frequ√™ncia em consultas, mas n√£o criar √≠ndices para um n√∫mero muito grande de colunas, pois isso pode ocupar espa√ßo e levar a tempo de reconstru√ß√£o do √≠ndice (Doc 4, P√°g. 130).\n",
      "3. √â importante considerar a flexibilidade e o espa√ßo de armazenamento ao escolher o tipo de dado para uma coluna (Doc 1, P√°g. 145).\n",
      "\n",
      "Essas conclus√µes s√£o baseadas nas discuss√µes sobre os tipos de dados inteiros (INT, TINYINT, SMALLINT) e a cria√ß√£o de √≠ndices em colunas.\n",
      "\n",
      "üìö Fontes consultadas: 4\n",
      "   [1] O'Reilly - PT - SQL Guia PraÃÅtico - Alice Zhao.pdf - p√°g. 145\n",
      "   [2] O'Reilly - PT - SQL Guia PraÃÅtico - Alice Zhao.pdf - p√°g. 14\n",
      "   [3] O'Reilly - PT - SQL Guia PraÃÅtico - Alice Zhao.pdf - p√°g. 13\n",
      "\n",
      "‚è±Ô∏è  Tempo: 1.78s\n"
     ]
    }
   ],
   "source": [
    "bot_rag.chat(\"Quais s√£o as principais conclus√µes ou recomenda√ß√µes?\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "660ef2e8-be0b-4ac7-a477-8e2a0eddf298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ Voc√™: Fale sobre o comando SELECT\n",
      "üîç Buscando contexto...\n",
      "\n",
      "ü§ñ Assistente: O comando SELECT √© um dos comandos mais importantes no SQL e √© usado para recuperar dados de uma tabela ou conjunto de tabelas. Ele √© usado para selecionar colunas espec√≠ficas de uma tabela e exibi-las em uma consulta.\n",
      "\n",
      "A sintaxe b√°sica do comando SELECT √©:\n",
      "\n",
      "```sql\n",
      "SELECT coluna1, coluna2, ...\n",
      "FROM tabela;\n",
      "```\n",
      "\n",
      "Onde:\n",
      "\n",
      "* `coluna1, coluna2, ...` s√£o as colunas que voc√™ deseja selecionar.\n",
      "* `tabela` √© o nome da tabela que voc√™ deseja consultar.\n",
      "\n",
      "Exemplo:\n",
      "\n",
      "```sql\n",
      "SELECT nome, idade\n",
      "FROM clientes;\n",
      "```\n",
      "\n",
      "Este comando selecionar√° apenas as colunas `nome` e `idade` da tabela `clientes`.\n",
      "\n",
      "Al√©m disso, o comando SELECT pode ser usado com operadores l√≥gicos, como `AND`, `OR` e `NOT`, para filtrar os resultados.\n",
      "\n",
      "Exemplo:\n",
      "\n",
      "```sql\n",
      "SELECT nome, idade\n",
      "FROM clientes\n",
      "WHERE idade > 18 AND cidade = 'S√£o Paulo';\n",
      "```\n",
      "\n",
      "Este comando selecionar√° apenas as colunas `nome` e `idade` da tabela `clientes`, onde a idade seja maior que 18 e a cidade seja S√£o Paulo.\n",
      "\n",
      "O comando SELECT tamb√©m pode ser usado com fun√ß√µes, como `SUM`, `AVG`, `MAX` e `MIN`, para calcular valores em uma coluna.\n",
      "\n",
      "Exemplo:\n",
      "\n",
      "```sql\n",
      "SELECT SUM(preco) AS total\n",
      "FROM produtos;\n",
      "```\n",
      "\n",
      "Este comando calcular√° a soma dos pre√ßos da tabela `produtos` e exibir√° o resultado como uma coluna chamada `total`.\n",
      "\n",
      "Em resumo, o comando SELECT √© uma ferramenta poderosa no SQL que permite selecionar dados de uma tabela ou conjunto de tabelas e exibi-los em uma consulta.\n",
      "\n",
      "üìö Fontes consultadas: 4\n",
      "   [1] O'Reilly - PT - SQL Guia PraÃÅtico - Alice Zhao.pdf - p√°g. 177\n",
      "   [2] O'Reilly - PT - SQL Guia PraÃÅtico - Alice Zhao.pdf - p√°g. 301\n",
      "   [3] O'Reilly - PT - SQL Guia PraÃÅtico - Alice Zhao.pdf - p√°g. 49\n",
      "\n",
      "‚è±Ô∏è  Tempo: 3.19s\n"
     ]
    }
   ],
   "source": [
    "bot_rag.chat(\"Fale sobre o comando SELECT\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5db26123-2857-4cf3-a2d7-cb07920896dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents_loaded': True,\n",
       " 'history_messages': 7,\n",
       " 'model': 'llama-3.1-8b-instant',\n",
       " 'embeddings': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       " 'vectors_in_index': 537}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot_rag.get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ba0170e-59bc-4207-a894-3652167b3cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos removidos!\n"
     ]
    }
   ],
   "source": [
    "bot_rag.clear_documents()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
